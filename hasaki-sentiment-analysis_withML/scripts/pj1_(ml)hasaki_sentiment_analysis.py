# -*- coding: utf-8 -*-
"""(fixed1)PJ1_(ML)Hasaki-sentiment-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19OVeRNJ7hZsXlAifxHhIwlNVgUw5uBXM
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

"""> # **Ph·∫ßn 1: Business Understanding**

## V·∫•n ƒë·ªÅ:
- X√¢y d·ª±ng h·ªá th·ªëng d·ª± ƒëo√°n ph·∫£n h·ªìi c·ªßa kh√°ch h√†ng v·ªÅ s·∫£n ph·∫©m/d·ªãch v·ª• c·ªßa Hasaki (t√≠ch c·ª±c, ti√™u c·ª±c, trung t√≠nh) t·ª´ c√°c ƒë√°nh gi√°.
- H·ªó tr·ª£ Hasaki v√† c√°c ƒë·ªëi t√°c c·∫£i thi·ªán s·∫£n ph·∫©m, d·ªãch v·ª• ƒë·ªÉ tƒÉng s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng.

## M·ª•c ti√™u:
- Ph√¢n lo·∫°i sentiment t·ª´ d·ªØ li·ªáu ƒë√°nh gi√° kh√°ch h√†ng.
- TƒÉng kh·∫£ nƒÉng ph√¢n t√≠ch v√† ph·∫£n h·ªìi nhanh ch√≥ng c√°c √Ω ki·∫øn c·ªßa kh√°ch h√†ng.

> # **Ph·∫ßn 2: Data Understanding**

## D·ªØ li·ªáu cung c·∫•p:
- San_pham.csv: Th√¥ng tin s·∫£n ph·∫©m (m√£, t√™n, gi√°, m√¥ t·∫£, ƒëi·ªÉm trung b√¨nh).
- Khach_hang.csv: Th√¥ng tin kh√°ch h√†ng (m√£, h·ªç t√™n).
- Danh_gia.csv: ƒê√°nh gi√° kh√°ch h√†ng (m√£ s·∫£n ph·∫©m, m√£ kh√°ch h√†ng, n·ªôi dung b√¨nh lu·∫≠n, s·ªë sao, ng√†y gi·ªù b√¨nh lu·∫≠n).


## H∆∞·ªõng ti·∫øp c·∫≠n:
- S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√°nh gi√° t·ª´ kh√°ch h√†ng l√†m input cho b√†i to√°n sentiment analysis.
- T·∫≠p trung v√†o d·ªØ li·ªáu text (c·ªôt Noi_dung_binh_luan) v√† s·ªë sao ƒë·ªÉ x√°c ƒë·ªãnh sentiment.
"""

danh_gia = pd.read_csv('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/data/Danh_gia.csv')
khach_hang = pd.read_csv('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/data/Khach_hang.csv')
san_pham = pd.read_csv('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/data/San_pham.csv')

# Merge d·ªØ li·ªáu
merged_data_1 = danh_gia.merge(khach_hang, on='ma_khach_hang', how='left')
final_data = merged_data_1.merge(san_pham, on='ma_san_pham', how='left')

# ƒê·ªïi t√™n c√°c c·ªôt
final_data.rename(columns={
    'ma_khach_hang': 'Ma_khach_hang',
    'ho_ten': 'Ho_ten',
    'ma_san_pham': 'Ma_san_pham',
    'ten_san_pham': 'Ten_san_pham',
    'noi_dung_binh_luan': 'Noi_dung_binh_luan',
    'ngay_binh_luan': 'Ngay_binh_luan',
    'so_sao': 'So_sao'
}, inplace=True)

# L·ª±a ch·ªçn c√°c c·ªôt c·∫ßn thi·∫øt
final_data = final_data[['Ma_khach_hang', 'Ho_ten', 'Ma_san_pham', 'Ten_san_pham',
                         'Noi_dung_binh_luan', 'Ngay_binh_luan',
                         'So_sao']]

# Hi·ªÉn th·ªã k·∫øt qu·∫£
final_data

"""## T·ªïng quan d·ªØ li·ªáu"""

print(final_data.info())

print(final_data.describe())

# Ki·ªÉm tra gi√° tr·ªã thi·∫øu
print(final_data.isnull().sum())

import pandas as pd
import matplotlib.pyplot as plt

# Ki·ªÉm tra s·ªë l∆∞·ª£ng Null v√† Non-Null trong c√°c c·ªôt
null_counts = {
    "C·ªôt": ["Noi_dung_binh_luan", "So_sao"],
    "Null": [final_data["Noi_dung_binh_luan"].isnull().sum(), final_data["So_sao"].isnull().sum()],
    "Non-Null": [final_data["Noi_dung_binh_luan"].notnull().sum(), final_data["So_sao"].notnull().sum()],
}

# Chuy·ªÉn t·ª´ ƒëi·ªÉn th√†nh DataFrame ƒë·ªÉ d·ªÖ thao t√°c
null_df = pd.DataFrame(null_counts)

# V·∫Ω bi·ªÉu ƒë·ªì
fig, ax = plt.subplots(figsize=(8, 5))

# T·∫°o bi·ªÉu ƒë·ªì stacked bar chart
bars_null = ax.bar(null_df["C·ªôt"], null_df["Null"], label="Null", color="skyblue")
bars_non_null = ax.bar(null_df["C·ªôt"], null_df["Non-Null"], label="Non-Null", color="orange", bottom=null_df["Null"])

# Hi·ªÉn th·ªã s·ªë li·ªáu l√™n bi·ªÉu ƒë·ªì (s·ªë Null)
for bar, count in zip(bars_null, null_df["Null"]):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height / 2, str(int(count)), ha='center', va='center', color="black")

# Hi·ªÉn th·ªã s·ªë li·ªáu l√™n bi·ªÉu ƒë·ªì (s·ªë Non-Null)
for bar, count in zip(bars_non_null, null_df["Non-Null"]):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2, str(int(count)), ha='center', va='center', color="black")

# Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ v√† nh√£n
ax.set_title("S·ªë l∆∞·ª£ng Null v√† Non-Null trong c√°c c·ªôt", fontsize=14)
ax.set_ylabel("S·ªë l∆∞·ª£ng", fontsize=12)
ax.set_xlabel("C·ªôt", fontsize=12)
ax.legend()
plt.xticks(rotation=0)
plt.tight_layout()

# Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
plt.show()

# X√≥a c√°c d√≤ng c√≥ gi√° tr·ªã Null trong c·ªôt Noi_dung_binh_luan
final_data = final_data.dropna(subset=["Noi_dung_binh_luan"])

# Ki·ªÉm tra k√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi x·ª≠ l√Ω
print("K√≠ch th∆∞·ªõc d·ªØ li·ªáu ban ƒë·∫ßu:", final_data.shape)
print("K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi lo·∫°i b·ªè Null:", final_data.shape)

import pandas as pd
import matplotlib.pyplot as plt

# Ki·ªÉm tra s·ªë l∆∞·ª£ng Null v√† Non-Null trong c√°c c·ªôt
null_counts = {
    "C·ªôt": ["Noi_dung_binh_luan", "So_sao"],
    "Null": [final_data["Noi_dung_binh_luan"].isnull().sum(), final_data["So_sao"].isnull().sum()],
    "Non-Null": [final_data["Noi_dung_binh_luan"].notnull().sum(), final_data["So_sao"].notnull().sum()],
}

# Chuy·ªÉn t·ª´ ƒëi·ªÉn th√†nh DataFrame ƒë·ªÉ d·ªÖ thao t√°c
null_df = pd.DataFrame(null_counts)

# V·∫Ω bi·ªÉu ƒë·ªì
fig, ax = plt.subplots(figsize=(8, 5))

# T·∫°o bi·ªÉu ƒë·ªì stacked bar chart
bars_null = ax.bar(null_df["C·ªôt"], null_df["Null"], label="Null", color="skyblue")
bars_non_null = ax.bar(null_df["C·ªôt"], null_df["Non-Null"], label="Non-Null", color="orange", bottom=null_df["Null"])

# Hi·ªÉn th·ªã s·ªë li·ªáu l√™n bi·ªÉu ƒë·ªì (s·ªë Null)
for bar, count in zip(bars_null, null_df["Null"]):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height / 2, str(int(count)), ha='center', va='center', color="black")

# Hi·ªÉn th·ªã s·ªë li·ªáu l√™n bi·ªÉu ƒë·ªì (s·ªë Non-Null)
for bar, count in zip(bars_non_null, null_df["Non-Null"]):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2, str(int(count)), ha='center', va='center', color="black")

# Thi·∫øt l·∫≠p ti√™u ƒë·ªÅ v√† nh√£n
ax.set_title("S·ªë l∆∞·ª£ng Null v√† Non-Null trong c√°c c·ªôt", fontsize=14)
ax.set_ylabel("S·ªë l∆∞·ª£ng", fontsize=12)
ax.set_xlabel("C·ªôt", fontsize=12)
ax.legend()
plt.xticks(rotation=0)
plt.tight_layout()

# Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
plt.show()

final_data['Ho_ten'].fillna('Kh√°ch h√†ng kh√¥ng r√µ t√™n', inplace=True)

"""## Xem ph√¢n ph·ªëi s·ªë li·ªáu"""

# Ki·ªÉm tra s·ªë l∆∞·ª£ng d√≤ng v·ªõi So_sao = 0
invalid_ratings = final_data[final_data["So_sao"] == 0]
print(f"S·ªë l∆∞·ª£ng d√≤ng v·ªõi So_sao = 0: {len(invalid_ratings)}")

# Hi·ªÉn th·ªã c√°c d√≤ng n√†y
print("\nC√°c d√≤ng c√≥ So_sao = 0:")
print(invalid_ratings)

import matplotlib.pyplot as plt

# ƒê·∫øm s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo t·ª´ng m·ª©c sao
rating_counts = final_data['So_sao'].value_counts()

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi s·ªë sao
plt.figure(figsize=(8, 5))
rating_counts.plot(kind='bar')
plt.title('Ph√¢n ph·ªëi s·ªë sao')
plt.xlabel('S·ªë sao')
plt.ylabel('S·ªë l∆∞·ª£ng ƒë√°nh gi√°')
plt.xticks(rotation=0)
plt.show()

# T·∫°o c·ªôt sentiment t·ª´ s·ªë sao
def get_sentiment(star):
    if star >= 4:
        return 'Positive'
    elif star == 3:
        return 'Neutral'
    else:
        return 'Negative'

final_data['Sentiment'] = final_data['So_sao'].apply(get_sentiment)

# Ph√¢n ph·ªëi sentiment
sentiment_counts = final_data['Sentiment'].value_counts()

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi sentiment
plt.figure(figsize=(8, 5))
sentiment_counts.plot(kind='bar', color=['green', 'orange', 'red'])
plt.title('Ph√¢n ph·ªëi sentiment')
plt.xlabel('Sentiment')
plt.ylabel('S·ªë l∆∞·ª£ng ƒë√°nh gi√°')
plt.show()

# Th√™m c·ªôt ƒë·ªô d√†i b√¨nh lu·∫≠n (s·ªë k√Ω t·ª±)
final_data['Comment_Length'] = final_data['Noi_dung_binh_luan'].apply(len)

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi ƒë·ªô d√†i b√¨nh lu·∫≠n
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 5))
final_data['Comment_Length'].hist(bins=50)
plt.title('Ph√¢n ph·ªëi ƒë·ªô d√†i b√¨nh lu·∫≠n')
plt.xlabel('ƒê·ªô d√†i b√¨nh lu·∫≠n')
plt.ylabel('S·ªë l∆∞·ª£ng')

plt.grid(False)

plt.show()

import numpy as np
import matplotlib.pyplot as plt

# √Åp d·ª•ng log transform (th√™m 1 ƒë·ªÉ tr√°nh log(0))
final_data['Log_Comment_Length'] = np.log1p(final_data['Comment_Length'])

# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi sau khi log transform
plt.figure(figsize=(8, 5))
final_data['Log_Comment_Length'].hist(bins=50)
plt.title('Ph√¢n ph·ªëi ƒë·ªô d√†i b√¨nh lu·∫≠n sau Log Transform')
plt.xlabel('Log(ƒê·ªô d√†i b√¨nh lu·∫≠n)')
plt.ylabel('S·ªë l∆∞·ª£ng')
plt.grid(False)
plt.show()

"""> # **Ph·∫ßn 3: Data Preparation**

## X·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n
"""

!pip install underthesea

import pandas as pd
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
import regex
import string

# LOAD EMOJICON
emoji_dict = {}
with open('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/files/emojicon.txt', 'r', encoding="utf8") as file:
    emoji_lst = file.read().split('\n')
    for line in emoji_lst:
        if '\t' in line:  # Ki·ªÉm tra n·∫øu d√≤ng ch·ª©a k√Ω t·ª± tab
            try:
                key, value = line.split('\t')
                emoji_dict[key.strip()] = value.strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
            except ValueError:
                print(f"Skipped invalid line: {line}")  # In ra n·∫øu d√≤ng kh√¥ng h·ª£p l·ªá

#################
# LOAD TEENCODE
teen_dict = {}
with open('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/files/teencode.txt', 'r', encoding="utf8") as file:
    teen_lst = file.read().split('\n')
    for line in teen_lst:
        if '\t' in line:  # Ki·ªÉm tra n·∫øu d√≤ng ch·ª©a k√Ω t·ª± tab
            try:
                key, value = line.split('\t')
                teen_dict[key.strip()] = value.strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
            except ValueError:
                print(f"Skipped invalid line: {line}")

###############
# LOAD TRANSLATE ENGLISH -> VNMESE
english_dict = {}
with open('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/files/english-vnmese.txt', 'r', encoding="utf8") as file:
    english_lst = file.read().split('\n')
    for line in english_lst:
        if '\t' in line:  # Ki·ªÉm tra n·∫øu d√≤ng ch·ª©a k√Ω t·ª± tab
            try:
                key, value = line.split('\t')
                english_dict[key.strip()] = value.strip()
            except ValueError:
                print(f"Skipped invalid line: {line}")

################
# LOAD wrong words
with open('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/files/wrong-word.txt', 'r', encoding="utf8") as file:
    wrong_lst = [word.strip() for word in file.read().split('\n') if word.strip()]  # Lo·∫°i b·ªè d√≤ng tr·ªëng v√† kho·∫£ng tr·∫Øng th·ª´a

#################
# LOAD STOPWORDS
with open('/content/drive/MyDrive/LDS7_K299_Online_HoaÃÄng NgoÃ£c ThuÃây ThuÃõoÃõng/Project_1/Cung_cap_HV/files/vietnamese-stopwords.txt', 'r', encoding="utf8") as file:
    stopwords_lst = [word.strip() for word in file.read().split('\n') if word.strip()]  # Lo·∫°i b·ªè d√≤ng tr·ªëng v√† kho·∫£ng tr·∫Øng th·ª´a

print("Emoji Dictionary Sample:", list(emoji_dict.items())[:5])
print("Teen Dictionary Sample:", list(teen_dict.items())[:5])
print("English Dictionary Sample:", list(english_dict.items())[:5])
print("Wrong Word List Sample:", wrong_lst[:5])
print("Stopwords List Sample:", stopwords_lst[:5])

def process_text(text, emoji_dict, teen_dict, wrong_lst):
    document = text.lower()
    document = document.replace("‚Äô", '')
    document = regex.sub(r'\.+', ".", document)

    # X·ª≠ l√Ω Emoji
    document = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(document))

    # X·ª≠ l√Ω Teen Code
    document = ' '.join(teen_dict[word] if word in teen_dict else word for word in document.split())

    # Lo·∫°i b·ªè t·ª´ sai
    document = ' '.join('' if word in wrong_lst else word for word in document.split())

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    document = regex.sub(r'\s+', ' ', document).strip()

    return document

def process_special_word(text):
    new_text = ''
    text_lst = text.split()
    i = 0
    if 'kh√¥ng' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            if word == 'kh√¥ng':
                next_idx = i + 1
                if next_idx <= len(text_lst) - 1:
                    word = word + '_' + text_lst[next_idx]
                i = next_idx + 1
            else:
                i += 1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()

example = '''M√¨nh r·∫•t th√≠ch hasaki va sp t·∫©y trang n√†y'''

print(example)

document = process_text(example, emoji_dict, teen_dict, wrong_lst)
document

# Chu·∫©n h√≥a unicode ti·∫øng vi·ªát
def loaddicchar():
    uniChars = "√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = '√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'.split(
        '|')
    charutf8 = "√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# ƒê∆∞a to√†n b·ªô d·ªØ li·ªáu qua h√†m n√†y ƒë·ªÉ chu·∫©n h√≥a l·∫°i
def covert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥',
        lambda x: dicchar[x.group()], txt)

def process_special_word(text):
    new_text = ''
    text_lst = text.split()
    i = 0
    while i <= len(text_lst) - 1:
        word = text_lst[i]
        # N·∫øu t·ª´ l√† "kh√¥ng", ki·ªÉm tra t·ª´ ti·∫øp theo
        if word == 'kh√¥ng' and i + 1 < len(text_lst):
            next_word = text_lst[i + 1]
            word = f"{word}_{next_word}"  # N·ªëi "kh√¥ng" v·ªõi t·ª´ ti·∫øp theo
            i += 1  # B·ªè qua t·ª´ ti·∫øp theo ƒë√£ ƒë∆∞·ª£c n·ªëi
        new_text += word + ' '
        i += 1
    return new_text.strip()

x = process_special_word(document)
x

import re
# H√†m ƒë·ªÉ chu·∫©n h√≥a c√°c t·ª´ c√≥ k√Ω t·ª± l·∫∑p
def normalize_repeated_characters(text):
    # Thay th·∫ø m·ªçi k√Ω t·ª± l·∫∑p li√™n ti·∫øp b·∫±ng m·ªôt k√Ω t·ª± ƒë√≥
    # V√≠ d·ª•: "l√≤nggggg" th√†nh "l√≤ng", "thi·ªátttt" th√†nh "thi·ªát"
    return re.sub(r'(.)\1+', r'\1', text)

# √Åp d·ª•ng h√†m chu·∫©n h√≥a cho vƒÉn b·∫£n
# print(normalize_repeated_characters(example))

document = normalize_repeated_characters(document)
print(document)

def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.', '')
        lst_word_type = ['N', 'Np', 'A', 'AB', 'V', 'VB', 'VY', 'R']
        sentence = ' '.join(word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document

document = process_postag_thesea(document)

document

def remove_stopword(text, stopwords):
    document = ' '.join(word for word in text.split() if word not in stopwords)
    return regex.sub(r'\s+', ' ', document).strip()  # X√≥a kho·∫£ng tr·∫Øng th·ª´a

document = remove_stopword(document, stopwords_lst)
document

# Load Emoji File
emoji_dict = {}
for line in emoji_lst:
    if '\t' in line:  # Ch·ªâ x·ª≠ l√Ω d√≤ng ch·ª©a k√Ω t·ª± tab
        key, value = line.split('\t')
        emoji_dict[key.strip()] = value.strip()

# Load Teen Code File
teen_dict = {}
for line in teen_lst:
    if '\t' in line:
        key, value = line.split('\t')
        teen_dict[key.strip()] = value.strip()

print("Processed emoji_dict:", list(emoji_dict.items())[:10])
print("Processed teen_dict:", list(teen_dict.items())[:10])

# L∆∞u n·ªôi dung g·ªëc c·ªßa c·ªôt tr∆∞·ªõc khi x·ª≠ l√Ω
final_data['Noi_dung_binh_luan_goc'] = final_data['Noi_dung_binh_luan']

# B∆∞·ªõc 1: X·ª≠ l√Ω text c∆° b·∫£n v·ªõi process_text
final_data['Noi_dung_binh_luan'] = final_data['Noi_dung_binh_luan'].apply(
    lambda x: process_text(x, emoji_dict, teen_dict, wrong_lst)
)

# B∆∞·ªõc 2: Chuy·ªÉn vƒÉn b·∫£n v·ªÅ d·∫°ng kh√¥ng d·∫•u
final_data['Noi_dung_binh_luan'] = final_data['Noi_dung_binh_luan'].apply(covert_unicode)

# B∆∞·ªõc 3: X·ª≠ l√Ω t·ª´ ƒë·∫∑c bi·ªát (v√≠ d·ª•: "kh√¥ng t·ªët" -> "kh√¥ng_t·ªët")
final_data['Noi_dung_binh_luan'] = final_data['Noi_dung_binh_luan'].apply(process_special_word)

# B∆∞·ªõc 4: Lo·∫°i b·ªè stopwords
final_data['Noi_dung_binh_luan'] = final_data['Noi_dung_binh_luan'].apply(
    lambda x: remove_stopword(x, stopwords_lst)
)

# B∆∞·ªõc 5: POS tagging v√† l·ªçc t·ª´ lo·∫°i c·∫ßn thi·∫øt
final_data['Noi_dung_binh_luan'] = final_data['Noi_dung_binh_luan'].apply(process_postag_thesea)

# T·∫°o b·∫£ng so s√°nh tr∆∞·ªõc v√† sau x·ª≠ l√Ω
comparison_table = final_data[['Noi_dung_binh_luan_goc', 'Noi_dung_binh_luan']].head(20)

comparison_table

"""# T·∫°o th√™m c√°c c·ªôt m·ªõi d·ª±a tr√™n vi·ªác ƒë·∫øm t·ª´/icon positive, negative, neutral"""

def find_words(text, word_list):
    text = text.lower()  # ƒê∆∞a vƒÉn b·∫£n v·ªÅ ch·ªØ th∆∞·ªùng
    word_count = 0
    found_words = []

    for word in word_list:
        if word in text:
            count = text.count(word)  # ƒê·∫øm s·ªë l·∫ßn t·ª´ xu·∫•t hi·ªán
            word_count += count
            found_words.append(word)

    return word_count, found_words

positive_words = [
    "th√≠ch", "t·ªët", "xu·∫•t s·∫Øc", "tuy·ªát v·ªùi", "·ªïn",
    "h√†i l√≤ng", "∆∞ng √Ω", "ho√†n h·∫£o", "ch·∫•t l∆∞·ª£ng", "nhanh",
    "ti·ªán l·ª£i", "d·ªÖ s·ª≠ d·ª•ng", "hi·ªáu qu·∫£", "·∫•n t∆∞·ª£ng",
    "n·ªïi b·∫≠t", "th√¢n thi·ªán", "cao c·∫•p", "ƒë·ªôc ƒë√°o", "r·∫•t t·ªët", "r·∫•t th√≠ch",
    "t·∫≠n t√¢m", "ƒë√°ng tin c·∫≠y", "ƒë·∫≥ng c·∫•p", "h·∫•p d·∫´n", "an t√¢m", "th√∫c ƒë·∫©y",
    "c·∫£m ƒë·ªông", "ph·ª•c v·ª• t·ªët", "l√†m h√†i l√≤ng", "g√¢y ·∫•n t∆∞·ª£ng", "n·ªïi tr·ªôi",
    "s√°ng t·∫°o", "ph√π h·ª£p", "t·∫≠n t√¢m", "hi·∫øm c√≥", "c·∫£i thi·ªán", "ho√† nh√£",
    "chƒÉm ch·ªâ", "c·∫©n th·∫≠n", "vui v·∫ª", "s√°ng s·ªßa", "h√†o h·ª©ng", "ƒëam m√™",
    "v·ª´a v·∫∑n", "ƒë√°ng ti·ªÅn", "ƒë√°ng mua", "ƒë·∫∑c s·∫Øc", "v∆∞·ª£t tr·ªôi", "d·ªÖ ch·ªãu",
    "ti·ªán nghi", "r·∫•t h·ªØu √≠ch", "an to√†n", "b·ªÅn", "d·ªÖ b·∫£o qu·∫£n", "x·ª©ng ƒë√°ng",
    "ngon", "xu·∫•t th·∫ßn", "l·ªãch s·ª±", "tinh t·∫ø", "ngƒÉn n·∫Øp", "s·∫°ch s·∫Ω",
    "·∫•m √°p", "kh√≠ch l·ªá", "ƒë√°ng y√™u", "ch·∫•t l∆∞·ª£ng cao", "vui l√≤ng", "r·∫•t nhanh",
    "tinh th·∫ßn t·ªët", "th√¢n √°i", "y√™u th√≠ch", "khuy·∫øn kh√≠ch", "b·∫•t ng·ªù", "ho√†n thi·ªán"
]

negative_words = [
    "k√©m", "t·ªá", "bu·ªìn", "ch√°n", "kh√¥ng d·ªÖ ch·ªãu", "kh√¥ng ch·∫•t l∆∞·ª£ng",
    "k√©m ch·∫•t l∆∞·ª£ng", "kh√¥ng th√≠ch", "kh√¥ng ·ªïn", "kh√¥ng h·ª£p",
    "kh√¥ng ƒë√°ng tin c·∫≠y", "kh√¥ng chuy√™n nghi·ªáp", "kh√¥ng ph·∫£n h·ªìi",
    "kh√¥ng an to√†n", "kh√¥ng ph√π h·ª£p", "kh√¥ng th√¢n thi·ªán", "kh√¥ng linh ho·∫°t",
    "kh√¥ng ƒë√°ng gi√°", "kh√¥ng ·∫•n t∆∞·ª£ng", "kh√¥ng t·ªët", "ch·∫≠m", "kh√≥ khƒÉn",
    "ph·ª©c t·∫°p", "kh√≥ ch·ªãu", "g√¢y kh√≥ d·ªÖ", "r∆∞·ªùm r√†", "th·∫•t b·∫°i", "t·ªìi t·ªá",
    "kh√≥ x·ª≠", "kh√¥ng th·ªÉ ch·∫•p nh·∫≠n", "t·ªìi t·ªá", "kh√¥ng r√µ r√†ng", "kh√¥ng ch·∫Øc ch·∫Øn",
    "r·ªëi r·∫Øm", "kh√¥ng ti·ªán l·ª£i", "kh√¥ng ƒë√°ng ti·ªÅn", "kh√¥ng h√†i l√≤ng",
    "kh√¥ng ƒë√°ng", "qu√° t·ªá", "r·∫•t t·ªá", "th·∫•t v·ªçng", "ch√°n", "t·ªá h·∫°i",
    "kinh kh·ªßng", "kh√¥ng ∆∞ng √Ω", "c·ª±c k·ª≥ t·ªá", "b·∫•t ti·ªán", "ph·∫£n c·∫£m",
    "l√£ng ph√≠", "x·∫•u", "kh√≥ hi·ªÉu", "thi·∫øu t·ªï ch·ª©c", "nguy hi·ªÉm", "d·ªü",
    "th·∫•t v·ªçng l·ªõn", "kh√¥ng th·ªèa ƒë√°ng", "b·∫•t m√£n", "b·ª±c m√¨nh", "r·ªëi lo·∫°n",
    "phi·ªÅn to√°i", "x·∫•u h·ªï", "m·∫•t th·ªùi gian", "c·∫©u th·∫£", "kh√¥ng ch·∫•p nh·∫≠n ƒë∆∞·ª£c",
    "qu√° ch·∫≠m", "t·ªá ƒë·∫øn m·ª©c kh√¥ng tin n·ªïi", "th·∫•t v·ªçng nghi√™m tr·ªçng", "qu√° r∆∞·ªùm r√†"
]

# T·ª´ ƒëi·ªÉn c√°c t·ª´ trung t√≠nh
neutral_words = [
    "b√¨nh th∆∞·ªùng", "trung b√¨nh", "kh√¥ng kh√°c bi·ªát", "v·ª´a ph·∫£i", "·ªïn ƒë·ªãnh",
    "ch·∫•p nh·∫≠n ƒë∆∞·ª£c", "kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát", "ƒë·∫°t y√™u c·∫ßu", "ƒë√°p ·ª©ng nhu c·∫ßu",
    "kh√¥ng g√¢y kh√≥ ch·ªãu", "t·∫°m ·ªïn", "kh√¥ng qu√° n·ªïi b·∫≠t", "kh√¥ng t·ªá", "d√πng ƒë∆∞·ª£c",
    "t∆∞∆°ng ƒë·ªëi", "v·ª´a ƒë·ªß", "trung l·∫≠p", "ph·ªï th√¥ng", "b√¨nh d·ªã", "th√¥ng th∆∞·ªùng",
    "kh√¥ng g√¢y ·∫£nh h∆∞·ªüng", "kh√¥ng sao", "th∆∞·ªùng", "t·∫°m ch·∫•p nh·∫≠n",
    "kh√¥ng t·ªët kh√¥ng x·∫•u", "·ªü m·ª©c trung b√¨nh", "kh√¥ng c√≥ g√¨ n·ªïi tr·ªôi",
    "kh√¥ng ƒë·∫∑c bi·ªát", "ƒë·ªÅu ƒë·ªÅu", "ƒë∆°n gi·∫£n", "v·ª´a v·∫∑n", "h·ª£p l√Ω",
    "b√¨nh ·ªïn", "kh√¥ng ƒë√°ng ch√∫ √Ω", "ƒë·∫°t ti√™u chu·∫©n", "kh√¥ng c√≥ g√¨ ph√†n n√†n",
    "·ªïn tho·∫£", "kh√¥ng n·ªïi b·∫≠t", "d·ªÖ ch·ªãu ·ªü m·ª©c v·ª´a ph·∫£i", "kh√¥ng ƒë·ªôt ph√°",
    "kh√¥ng v∆∞·ª£t tr·ªôi", "th·ª±c t·∫ø", "kh√¥ng qu√° t·ªët", "kh√¥ng qu√° x·∫•u",
    "ƒë∆∞·ª£c ch·∫•p nh·∫≠n", "kh√¥ng l√†m phi·ªÅn", "v·ª´a s·ª©c", "·ªü m·ª©c v·ª´a ph·∫£i",
    "tr√≤n vai", "kh√¥ng c·∫ßn thi·∫øt ph·∫£i c·∫£i thi·ªán", "kh√¥ng l√†m h√†i l√≤ng nh∆∞ng c≈©ng kh√¥ng g√¢y th·∫•t v·ªçng",
    "kh√¥ng qu√° t·ªá", "trong ng∆∞·ª°ng b√¨nh th∆∞·ªùng", "kh√¥ng ph·∫£i l√† t·ªët nh·∫•t",
    "kh√¥ng g√¢y ·∫•n t∆∞·ª£ng m·∫°nh", "kh√¥ng ƒë·ªß n·ªïi b·∫≠t", "v·∫´n ·ªïn", "ch·ªâ ƒë·ªß d√πng",
    "kh√¥ng ho√†n h·∫£o", "h∆°i nh·∫°t nh·∫Ωo", "c∆° b·∫£n", "kh√¥ng t·∫°o s·ª± kh√°c bi·ªát l·ªõn"
]

# H√†m t√≠nh to√°n sentiment counts (t√≠ch c·ª±c, ti√™u c·ª±c, trung t√≠nh)
def calculate_sentiment_counts_with_neutral(row):
    positive_count, _ = find_words(row, positive_words)
    negative_count, _ = find_words(row, negative_words)
    neutral_count, _ = find_words(row, neutral_words)
    return positive_count, negative_count, neutral_count

final_data[['positive_count', 'negative_count', 'neutral_count']] = final_data['Noi_dung_binh_luan'].apply(
    lambda x: pd.Series(calculate_sentiment_counts_with_neutral(x))
)

final_data[['Noi_dung_binh_luan', 'positive_count', 'negative_count', 'neutral_count']].head()

positive_emojis = [
    "üòÑ", "üòÉ", "üòÄ", "üòÅ", "üòÜ", "üòÖ", "ü§£", "üòÇ", "üôÇ", "üôÉ",
    "üòâ", "üòä", "üòá", "ü•∞", "üòç", "ü§©", "üòò", "üòó", "üòö", "üòô",
    "üòã", "üòõ", "üòú", "ü§™", "üòù", "ü§ó", "ü§≠", "ü•≥", "üòå", "üòé",
    "ü§ì", "üßê", "üëç", "ü§ù", "üôå", "üëè", "üëã", "ü§ô", "‚úã", "üñêÔ∏è",
    "üëå", "ü§û", "‚úåÔ∏è", "ü§ü", "üëà", "üëâ", "üëÜ", "üëá", "‚òùÔ∏è", "üíö",
    "üíñ", "üíû", "üíï", "üíì", "üíó", "üíù", "üíò", "‚ù£Ô∏è", "üíü", "üíå",
    "‚ù§Ô∏è", "üß°", "üíõ", "üíú", "üíô", "üíö", "üíØ", "üî•", "‚ú®", "üåü",
    "‚≠ê", "üå†", "üéâ", "üéä", "üéà", "üéÅ", "üéÄ", "ü•á", "üèÜ", "üèÖ",
    "üíê", "üå∏", "üå∫", "üåª", "üåº", "üå∑", "üåπ", "üåû", "‚òÄÔ∏è", "‚òÄ",
    "üïäÔ∏è", "üé∂", "üéµ", "üéº", "ü•Ç", "üçæ", "üçª", "ü•∞", "üòò", "üòá"
]

negative_emojis = [
    "üòû", "üòî", "üôÅ", "‚òπÔ∏è", "üòï", "üò¢", "üò≠", "üòñ", "üò£", "üò©",
    "üò†", "üò°", "ü§¨", "üò§", "üò∞", "üò®", "üò±", "üò™", "üòì", "ü•∫",
    "üòí", "üôÑ", "üòë", "üò¨", "üò∂", "ü§Ø", "üò≥", "ü§¢", "ü§Æ", "ü§ï",
    "ü•¥", "ü§î", "üò∑", "üôÖ‚Äç‚ôÇÔ∏è", "üôÖ‚Äç‚ôÄÔ∏è", "üôá‚Äç‚ôÇÔ∏è",
    "üôá‚Äç‚ôÄÔ∏è", "ü§¶‚Äç‚ôÇÔ∏è", "ü§¶‚Äç‚ôÄÔ∏è", "ü§∑‚Äç‚ôÇÔ∏è", "ü§∑‚Äç‚ôÄÔ∏è", "ü§¢", "ü§ß", "ü§®",
    "ü§´", "üëé", "üëä", "‚úä", "ü§õ", "ü§ú", "ü§ö", "üñï", "üò†", "üò°",
    "üíî", "‚ùå", "üö´", "üî¥", "üõë", "‚ö†Ô∏è", "‚ÄºÔ∏è", "üòß", "üòü", "üòû",
    "üò£", "üò´", "üòñ", "üòû", "üò§", "üò©", "ü•µ", "ü•∂", "üôÉ", "ü•¥"
]

neutral_emojis = [
    "üòê", "üò∂", "ü§î", "ü§´", "ü§®", "üôÑ", "üßê",
    "‚úã", "üëä", "üëâ", "üëà", "üëÜ", "üëá", "‚òùÔ∏è", "ü§ô", "ü§ù", "üëã",
    "üñêÔ∏è", "‚úåÔ∏è", "üëå", "ü§û", "ü§ü", "ü§Ø", "üï∂Ô∏è", "üìö", "üìñ", "üìí",
    "üìì", "üìî", "üìù", "‚úèÔ∏è", "üíº", "üëú", "üí°", "üìä", "üìà", "üìâ",
    "üìÖ", "üïó", "üïò", "üì∑", "üìπ", "üé•", "üé¨", "üïπÔ∏è", "üéÆ", "üìã",
    "üìé", "üìê", "üìå", "üìç", "üóëÔ∏è", "üóÇÔ∏è", "üíæ", "üì§", "üì•", "üí≥"
]

def calculate_sentiment_counts_with_emojis(row):
    # ƒê·∫øm t·ª´
    positive_word_count, _ = find_words(row, positive_words)
    negative_word_count, _ = find_words(row, negative_words)
    neutral_word_count, _ = find_words(row, neutral_words)

    # ƒê·∫øm emoji
    positive_emoji_count, _ = find_words(row, positive_emojis)
    negative_emoji_count, _ = find_words(row, negative_emojis)
    neutral_emoji_count, _ = find_words(row, neutral_emojis)

    # T·ªïng s·ªë t√≠ch c·ª±c, ti√™u c·ª±c, trung t√≠nh
    positive_count = positive_word_count + positive_emoji_count
    negative_count = negative_word_count + negative_emoji_count
    neutral_count = neutral_word_count + neutral_emoji_count

    return positive_count, negative_count, neutral_count

final_data[['positive_count', 'negative_count', 'neutral_count']] = final_data['Noi_dung_binh_luan'].apply(
    lambda x: pd.Series(calculate_sentiment_counts_with_emojis(x))
)

# Ki·ªÉm tra k·∫øt qu·∫£
print(final_data[['Noi_dung_binh_luan', 'positive_count', 'negative_count', 'neutral_count']].head())

final_data[['Noi_dung_binh_luan', 'positive_count', 'negative_count', 'neutral_count']].head()

# pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from wordcloud import STOPWORDS

# ƒê·ªãnh nghƒ©a c√°c t·ª´ d·ª´ng (n·∫øu c·∫ßn)
custom_stopwords = set(STOPWORDS)

# V·∫Ω Word Cloud cho t·ª´ t√≠ch c·ª±c
positive_text = ' '.join(final_data['Noi_dung_binh_luan'][final_data['positive_count'] > 0])
wordcloud_positive = WordCloud(
    width=800,
    height=400,
    background_color='white',
    max_words=50,  # Hi·ªÉn th·ªã t·ªëi ƒëa 50 t·ª´
    stopwords=custom_stopwords
).generate(positive_text)

plt.figure(figsize=(10, 5))
plt.title("Word Cloud - Positive Words")
plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis("off")
plt.show()

# V·∫Ω Word Cloud cho t·ª´ ti√™u c·ª±c
negative_text = ' '.join(final_data['Noi_dung_binh_luan'][final_data['negative_count'] > 0])
wordcloud_negative = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='Reds',  # M√†u s·∫Øc cho t·ª´ ti√™u c·ª±c
    max_words=50,  # Hi·ªÉn th·ªã t·ªëi ƒëa 50 t·ª´
    stopwords=custom_stopwords
).generate(negative_text)

plt.figure(figsize=(10, 5))
plt.title("Word Cloud - Negative Words")
plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis("off")
plt.show()

# V·∫Ω Word Cloud cho t·ª´ trung t√≠nh
neutral_text = ' '.join(final_data['Noi_dung_binh_luan'][(final_data['neutral_count'] > 0) &
                                                         (final_data['positive_count'] == 0) &
                                                         (final_data['negative_count'] == 0)])
wordcloud_neutral = WordCloud(
    width=800,
    height=400,
    background_color='white',
    colormap='winter',
    max_words=50,  # Hi·ªÉn th·ªã t·ªëi ƒëa 50 t·ª´
    stopwords=custom_stopwords
).generate(neutral_text)

plt.figure(figsize=(10, 5))
plt.title("Word Cloud - Neutral Words")
plt.imshow(wordcloud_neutral, interpolation='bilinear')
plt.axis("off")
plt.show()

# T·∫°o b·∫£ng th·ªëng k√™ s·ªë l∆∞·ª£ng t·ª´ theo sentiment
sentiment_summary = final_data.groupby('Sentiment')[['positive_count', 'negative_count', 'neutral_count']].sum().reset_index()

# S·∫Øp x·∫øp th·ª© t·ª± Sentiment n·∫øu c·∫ßn
sentiment_order = ['Positive', 'Neutral', 'Negative']
sentiment_summary['Sentiment'] = pd.Categorical(sentiment_summary['Sentiment'], categories=sentiment_order, ordered=True)
sentiment_summary = sentiment_summary.sort_values('Sentiment')

# Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£
print("B·∫£ng Th·ªëng K√™ S·ªë L∆∞·ª£ng T·ª´ Theo Sentiment:")
print(sentiment_summary)

# V·∫Ω bi·ªÉu ƒë·ªì tr·ª±c quan h√≥a
import matplotlib.pyplot as plt

# ƒê·∫∑t sentiment l√†m index ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
sentiment_summary.set_index('Sentiment', inplace=True)

# V·∫Ω bi·ªÉu ƒë·ªì stacked bar chart
sentiment_summary.plot(kind='bar', stacked=True, figsize=(10, 6), color=['green', 'orange', 'blue'])

plt.title('T·ªïng s·ªë l∆∞·ª£ng t·ª´ t√≠ch c·ª±c, ti√™u c·ª±c, trung t√≠nh theo sentiment')
plt.xlabel('Sentiment')
plt.ylabel('S·ªë l∆∞·ª£ng t·ª´')
plt.legend(['Positive Words', 'Negative Words', 'Neutral Words'])
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""## Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho m√¥ h√¨nh h·ªçc m√°y"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split

# 1. Chu·∫©n b·ªã d·ªØ li·ªáu
# L·∫•y d·ªØ li·ªáu vƒÉn b·∫£n (Noi_dung_binh_luan) v√† nh√£n (Sentiment)
X = final_data['Noi_dung_binh_luan']  # VƒÉn b·∫£n ƒë·∫ßu v√†o
y = final_data['Sentiment']  # Nh√£n c·∫£m x√∫c

# 2. CountVectorizer
print("=== CountVectorizer ===")
count_vectorizer = CountVectorizer(max_features=5000)  # Ch·ªçn t·ªëi ƒëa 5000 t·ª´ ph·ªï bi·∫øn nh·∫•t
X_count = count_vectorizer.fit_transform(X)
print("K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau CountVectorizer:", X_count.shape)

# 3. TF-IDF Vectorizer
print("\n=== TF-IDF Vectorizer ===")
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Ch·ªçn t·ªëi ƒëa 5000 t·ª´ ph·ªï bi·∫øn nh·∫•t
X_tfidf = tfidf_vectorizer.fit_transform(X)
print("K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau TF-IDF:", X_tfidf.shape)

# 4. Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm th·ª≠ (80% train - 20% test)
X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42, stratify=y)
X_train_count, X_test_count, _, _ = train_test_split(X_count, y, test_size=0.2, random_state=42, stratify=y)

# 5. Ki·ªÉm tra k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu sau khi t√°ch
print("\n=== K√≠ch th∆∞·ªõc t·∫≠p TF-IDF ===")
print("T·∫≠p hu·∫•n luy·ªán:", X_train_tfidf.shape)
print("T·∫≠p ki·ªÉm th·ª≠:", X_test_tfidf.shape)

print("\n=== K√≠ch th∆∞·ªõc t·∫≠p CountVectorizer ===")
print("T·∫≠p hu·∫•n luy·ªán:", X_train_count.shape)
print("T·∫≠p ki·ªÉm th·ª≠:", X_test_count.shape)

from imblearn.over_sampling import SMOTE
from collections import Counter
import matplotlib.pyplot as plt

# ƒêi·ªÅu ch·ªânh sampling_strategy ƒë·ªÉ ch·ªâ tƒÉng s·ªë l∆∞·ª£ng l·ªõp Negative v√† Neutral
sampling_strategy = {
    'Negative': 11936,  # TƒÉng s·ªë l∆∞·ª£ng l·ªõp Negative l√™n 80% c·ªßa l·ªõp positive
    'Positive': 14920,  # Gi·ªØ nguy√™n l·ªõp Positive
    'Neutral': 7460}    # TƒÉng s·ªë l∆∞·ª£ng l·ªõp Neutral l√™n 50% c·ªßa l·ªõp positive

smote = SMOTE(random_state=42, sampling_strategy=sampling_strategy)

# T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng
X_train_tfidf_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Ki·ªÉm tra l·∫°i ph√¢n ph·ªëi nh√£n sau khi √°p d·ª•ng SMOTE
print("Ph√¢n ph·ªëi nh√£n tr∆∞·ªõc khi √°p d·ª•ng SMOTE:", Counter(y_train))
print("Ph√¢n ph·ªëi nh√£n sau khi √°p d·ª•ng SMOTE:", Counter(y_train_resampled))

# H√†m v·∫Ω ph√¢n ph·ªëi nh√£n tr∆∞·ªõc v√† sau khi √°p d·ª•ng SMOTE
def plot_distribution(before, after, labels):
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))

    ax[0].bar(labels, before.values(), color='skyblue')
    ax[0].set_title('Tr∆∞·ªõc khi √°p d·ª•ng SMOTE')

    ax[1].bar(labels, after.values(), color='orange')
    ax[1].set_title('Sau khi √°p d·ª•ng SMOTE')

    plt.show()

# Ch·ªâ ƒë·ªãnh c√°c nh√£n l·ªõp
labels = ['Negative', 'Positive', 'Neutral']
plot_distribution(Counter(y_train), Counter(y_train_resampled), labels)

"""> # **Ph·∫ßn 4: Modeling & Evaluation**

## Machine learning
- S·ª≠ d·ª•ng 3 m√¥ h√¨nh: LogisticRegression, Random Forest, SVM
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import time
from wordcloud import WordCloud

# H√†m chu·∫©n b·ªã d·ªØ li·ªáu
def prepare_data():
   X = final_data['Noi_dung_binh_luan']  # C·ªôt n·ªôi dung b√¨nh lu·∫≠n
   y = final_data['Sentiment']  # C·ªôt nh√£n c·∫£m x√∫c (s·ªë sao)

   # Bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√†nh vector
   vectorizer = CountVectorizer()
   X_encoded = vectorizer.fit_transform(X)

   # Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
   X_train, X_test, y_train, y_test = train_test_split(
       X_encoded, y, test_size=0.2, random_state=42
   )
   train_idx, test_idx = y_train.index, y_test.index
   return X_train, X_test, y_train, y_test, train_idx, test_idx

# 2. Hu·∫•n luy·ªán m√¥ h√¨nh
def train_model(model, X_train, y_train):
   start_time = time.time()
   model.fit(X_train, y_train)
   end_time = time.time()
   elapsed_time = end_time - start_time
   return model, elapsed_time

# 3. ƒê√°nh gi√° m√¥ h√¨nh
def evaluate_model(model, X_train, y_train, X_test, y_test):
   # D·ª± ƒëo√°n tr√™n t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
   y_train_pred = model.predict(X_train)
   y_test_pred = model.predict(X_test)

   # Ki·ªÉm tra l·∫°i ki·ªÉu d·ªØ li·ªáu c·ªßa y_train_pred v√† y_test_pred
   if isinstance(y_train_pred, (int, np.int64)):
       y_train_pred = np.array([y_train_pred])  # Chuy·ªÉn ƒë·ªïi th√†nh m·∫£ng n·∫øu l√† s·ªë nguy√™n ƒë∆°n
   if isinstance(y_test_pred, (int, np.int64)):
       y_test_pred = np.array([y_test_pred])  # Chuy·ªÉn ƒë·ªïi th√†nh m·∫£ng n·∫øu l√† s·ªë nguy√™n ƒë∆°n

   # T√≠nh to√°n ƒë·ªô ch√≠nh x√°c v√† in b√°o c√°o ph√¢n lo·∫°i
   train_accuracy = accuracy_score(y_train, y_train_pred)
   test_accuracy = accuracy_score(y_test, y_test_pred)

   print(f"Training Accuracy: {train_accuracy:.4f}")
   print(classification_report(y_train, y_train_pred, target_names=class_labels))

   print(f"Test Accuracy: {test_accuracy:.4f}")
   print(classification_report(y_test, y_test_pred, target_names=class_labels))

   return y_test_pred

# 4. V·∫Ω confusion matrix
def plot_confusion_matrix(cm, model_name, classes):
   plt.figure(figsize=(8, 6))
   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
   plt.title(f"Confusion Matrix for {model_name}")
   plt.xlabel("Predicted Labels")
   plt.ylabel("True Labels")
   plt.show()

# 5. T·∫°o b·∫£ng k·∫øt qu·∫£ d·ª± ƒëo√°n
def create_prediction_table(model, name, X_test, y_test_pred, test_idx):
   data_test = final_data.loc[test_idx].copy()  # L·∫•y l·∫°i c√°c d√≤ng t·ª´ t·∫≠p test b·∫±ng test_idx
   data_test['Predicted_label'] = y_test_pred  # Nh√£n d·ª± ƒëo√°n
   data_test['Sentiment'] = le.inverse_transform(data_test['Predicted_label'])
   data_test['Model'] = name  # Th√™m c·ªôt ghi t√™n m√¥ h√¨nh
   print(f"Sample Predictions for {name}:")
   print(data_test[['Noi_dung_binh_luan', 'So_sao', 'Predicted_label', 'Sentiment']].head())
   print("=" * 60)
   return data_test[['Noi_dung_binh_luan', 'So_sao', 'Predicted_label', 'Sentiment', 'Model']]

# Ph√¢n t√≠ch chi ti·∫øt s·∫£n ph·∫©m c·ª• th·ªÉ
def analyze_product_details(final_data):
    # L·ªçc ra c√°c s·∫£n ph·∫©m v√† ph√¢n t√≠ch c√°c b√¨nh lu·∫≠n t√≠ch c·ª±c v√† ti√™u c·ª±c
    positive_comments = final_data[final_data['Sentiment'] == 'Positive']['Noi_dung_binh_luan']
    negative_comments = final_data[final_data['Sentiment'] == 'Negative']['Noi_dung_binh_luan']

    # T·∫°o word cloud cho c√°c b√¨nh lu·∫≠n t√≠ch c·ª±c
    positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(positive_comments))
    plt.figure(figsize=(8, 6))
    plt.imshow(positive_wordcloud, interpolation='bilinear')
    plt.title("WordCloud for Positive Comments")
    plt.axis("off")
    plt.show()

    # T·∫°o word cloud cho c√°c b√¨nh lu·∫≠n ti√™u c·ª±c
    negative_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(negative_comments))
    plt.figure(figsize=(8, 6))
    plt.imshow(negative_wordcloud, interpolation='bilinear')
    plt.title("WordCloud for Negative Comments")
    plt.axis("off")
    plt.show()

    # T√≠nh t·ªïng s·ªë nh·∫≠n x√©t t√≠ch c·ª±c v√† ti√™u c·ª±c
    positive_count = len(positive_comments)
    negative_count = len(negative_comments)

    print(f"Total Positive Comments: {positive_count}")
    print(f"Total Negative Comments: {negative_count}")

    # Tr·∫£ v·ªÅ d·ªØ li·ªáu chi ti·∫øt
    return positive_count, negative_count, positive_wordcloud, negative_wordcloud

# G·ªçi h√†m ƒë·ªÉ l·∫•y d·ªØ li·ªáu
X_train, X_test, y_train, y_test, train_idx, test_idx = prepare_data()

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import pandas as pd
import time

# M√£ h√≥a nh√£n
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

# L·∫•y nh√£n l·ªõp
class_labels = le.classes_
print("Class labels:", class_labels)

# Ki·ªÉm tra nh√£n ƒë√£ m√£ h√≥a
print("Encoded labels for y_train:", set(y_train))
print("Encoded labels for y_test:", set(y_test))

# Kh·ªüi t·∫°o c√°c m√¥ h√¨nh
models = {
    "Logistic Regression": LogisticRegression(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM": SVC(random_state=42, probability=True)
}

# Kh·ªüi t·∫°o danh s√°ch l∆∞u k·∫øt qu·∫£ t·ªïng h·ª£p
comparison_results = []

# H√†m hu·∫•n luy·ªán m√¥ h√¨nh
def train_model(model, X_train, y_train):
    start_time = time.time()
    model.fit(X_train, y_train)
    elapsed_time = time.time() - start_time
    return model, elapsed_time

# H√†m ƒë√°nh gi√° m√¥ h√¨nh
def evaluate_model(model, X_test):
    start_time = time.time()
    y_test_pred = model.predict(X_test)
    elapsed_time = time.time() - start_time
    return y_test_pred, elapsed_time

# H√†m v·∫Ω confusion matrix
def plot_confusion_matrix(cm, model_name, class_labels):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.tight_layout()
    plt.show()

# Hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh
for name, model in models.items():
    print(f"Training {name}...")

    # Hu·∫•n luy·ªán m√¥ h√¨nh v√† t√≠nh th·ªùi gian hu·∫•n luy·ªán
    model, train_time = train_model(model, X_train, y_train)
    print(f"Training time for {name}: {train_time:.2f} seconds\n")

    # D·ª± ƒëo√°n tr√™n t·∫≠p test v√† ƒëo th·ªùi gian
    y_test_pred, test_time = evaluate_model(model, X_test)
    print(f"Testing time for {name}: {test_time:.2f} seconds\n")

    # T√≠nh to√°n c√°c ch·ªâ s·ªë hi·ªáu su·∫•t
    y_train_pred = model.predict(X_train)
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    classification = classification_report(y_test, y_test_pred, target_names=class_labels, output_dict=True)

    # In Training Accuracy v√† Test Accuracy
    print(f"Training Accuracy: {train_accuracy:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}\n")

    # T·∫°o DataFrame cho b·∫£ng ch·ªâ s·ªë chi ti·∫øt
    metrics = []
    for label in class_labels:
        metrics.append({
            "Class": label,
            "Precision": classification[label]["precision"],
            "Recall": classification[label]["recall"],
            "F1-Score": classification[label]["f1-score"],
            "Support": int(classification[label]["support"])
        })

    # Th√™m Macro Avg v√† Weighted Avg
    metrics_df = pd.concat([
        pd.DataFrame(metrics),
        pd.DataFrame([{
            "Class": "Macro Avg",
            "Precision": classification["macro avg"]["precision"],
            "Recall": classification["macro avg"]["recall"],
            "F1-Score": classification["macro avg"]["f1-score"],
            "Support": sum(metrics_df["Support"])
        }, {
            "Class": "Weighted Avg",
            "Precision": classification["weighted avg"]["precision"],
            "Recall": classification["weighted avg"]["recall"],
            "F1-Score": classification["weighted avg"]["f1-score"],
            "Support": sum(metrics_df["Support"])
        }])
    ], ignore_index=True)

    # Th√™m th·ªùi gian ki·ªÉm tra v√†o b·∫£ng
    metrics_df["Testing Time (s)"] = test_time

    # Hi·ªÉn th·ªã b·∫£ng ch·ªâ s·ªë
    print(f"Performance Metrics for {name}:")
    display(metrics_df)

    # T√≠nh to√°n v√† v·∫Ω confusion matrix
    cm = confusion_matrix(y_test, y_test_pred, labels=range(len(class_labels)))
    plot_confusion_matrix(cm, name, class_labels)

    # L∆∞u k·∫øt qu·∫£ v√†o danh s√°ch so s√°nh
    comparison_results.append({
        "Model": name,
        "Train Accuracy": train_accuracy,
        "Test Accuracy": test_accuracy,
        "Macro F1-Score": classification['macro avg']['f1-score'],
        "Test Time (seconds)": test_time
    })

# T·∫°o DataFrame t·ª´ k·∫øt qu·∫£ so s√°nh
comparison_df = pd.DataFrame(comparison_results)

# Hi·ªÉn th·ªã b·∫£ng t·ªïng h·ª£p hi·ªáu su·∫•t
print("\nB·∫£ng So S√°nh Hi·ªáu Su·∫•t C√°c M√¥ H√¨nh:")
display(comparison_df)

"""> # **Ph·∫ßn 5: Analyze & Report**

## 1. Ph√¢n t√≠ch s·∫£n ph·∫©m
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer

def analyze_product(product_id, product_name, final_data):
    # L·ªçc d·ªØ li·ªáu c·ªßa s·∫£n ph·∫©m c·ª• th·ªÉ
    # Chuy·ªÉn c·ªôt 'Ma_san_pham' v·ªÅ ki·ªÉu chu·ªói ƒë·ªÉ so s√°nh
    final_data['Ma_san_pham'] = final_data['Ma_san_pham'].astype(str)
    product_id = str(product_id)  # ƒê·∫£m b·∫£o product_id c≈©ng l√† chu·ªói
    final_data['Ma_san_pham'] = final_data['Ma_san_pham'].str.strip()
    product_id = product_id.strip()

    product_data = final_data[final_data['Ma_san_pham'] == product_id]

    # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu cho s·∫£n ph·∫©m kh√¥ng
    if product_data.empty:
        print(f"Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho s·∫£n ph·∫©m {product_name} (M√£: {product_id})")
        return

    # L·∫•y s·ªë nh·∫≠n x√©t t√≠ch c·ª±c, ti√™u c·ª±c v√† trung t√≠nh
    positive_reviews = product_data[product_data['Sentiment'] == 'Positive']
    negative_reviews = product_data[product_data['Sentiment'] == 'Negative']
    neutral_reviews = product_data[product_data['Sentiment'] == 'Neutral']

    positive_count = len(positive_reviews)
    negative_count = len(negative_reviews)
    neutral_count = len(neutral_reviews)

    print(f"S·∫£n ph·∫©m: {product_name} (M√£ s·∫£n ph·∫©m: {product_id})")
    print(f"S·ªë nh·∫≠n x√©t t√≠ch c·ª±c: {positive_count}")
    print(f"S·ªë nh·∫≠n x√©t ti√™u c·ª±c: {negative_count}")
    print(f"S·ªë nh·∫≠n x√©t trung t√≠nh: {neutral_count}")

    # T·∫°o wordcloud cho nh·∫≠n x√©t t√≠ch c·ª±c
    positive_text = " ".join(positive_reviews['Noi_dung_binh_luan'])
    if positive_text:  # Ki·ªÉm tra n·∫øu c√≥ d·ªØ li·ªáu
        wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
    else:
        wordcloud_pos = None  # Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o wordcloud

    # T·∫°o wordcloud cho nh·∫≠n x√©t ti√™u c·ª±c
    negative_text = " ".join(negative_reviews['Noi_dung_binh_luan'])
    if negative_text:  # Ki·ªÉm tra n·∫øu c√≥ d·ªØ li·ªáu
        wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(negative_text)
    else:
        wordcloud_neg = None  # Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o wordcloud

    # T·∫°o wordcloud cho nh·∫≠n x√©t trung t√≠nh
    neutral_text = " ".join(neutral_reviews['Noi_dung_binh_luan'])
    if neutral_text:  # Ki·ªÉm tra n·∫øu c√≥ d·ªØ li·ªáu
        wordcloud_neutral = WordCloud(width=800, height=400, background_color='white').generate(neutral_text)
    else:
        wordcloud_neutral = None  # Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o wordcloud

    # Hi·ªÉn th·ªã wordcloud t√≠ch c·ª±c, ti√™u c·ª±c v√† trung t√≠nh n·∫øu c√≥ d·ªØ li·ªáu
    plt.figure(figsize=(18, 6))

    if wordcloud_pos:  # N·∫øu c√≥ wordcloud t√≠ch c·ª±c, hi·ªÉn th·ªã
        plt.subplot(1, 3, 1)
        plt.imshow(wordcloud_pos, interpolation="bilinear")
        plt.title(f"Wordcloud - T√≠ch c·ª±c ({positive_count})")
        plt.axis("off")
    else:
        plt.subplot(1, 3, 1)
        plt.text(0.5, 0.5, 'Kh√¥ng c√≥ nh·∫≠n x√©t t√≠ch c·ª±c', ha='center', va='center', fontsize=12, color='red')
        plt.title(f"Wordcloud - T√≠ch c·ª±c ({positive_count})")
        plt.axis("off")

    if wordcloud_neg:  # N·∫øu c√≥ wordcloud ti√™u c·ª±c, hi·ªÉn th·ªã
        plt.subplot(1, 3, 2)
        plt.imshow(wordcloud_neg, interpolation="bilinear")
        plt.title(f"Wordcloud - Ti√™u c·ª±c ({negative_count})")
        plt.axis("off")
    else:
        plt.subplot(1, 3, 2)
        plt.text(0.5, 0.5, 'Kh√¥ng c√≥ nh·∫≠n x√©t ti√™u c·ª±c', ha='center', va='center', fontsize=12, color='red')
        plt.title(f"Wordcloud - Ti√™u c·ª±c ({negative_count})")
        plt.axis("off")

    if wordcloud_neutral:  # N·∫øu c√≥ wordcloud trung t√≠nh, hi·ªÉn th·ªã
        plt.subplot(1, 3, 3)
        plt.imshow(wordcloud_neutral, interpolation="bilinear")
        plt.title(f"Wordcloud - Trung t√≠nh ({neutral_count})")
        plt.axis("off")
    else:
        plt.subplot(1, 3, 3)
        plt.text(0.5, 0.5, 'Kh√¥ng c√≥ nh·∫≠n x√©t trung t√≠nh', ha='center', va='center', fontsize=12, color='red')
        plt.title(f"Wordcloud - Trung t√≠nh ({neutral_count})")
        plt.axis("off")

    plt.show()

    # C√°c t·ª´ kh√≥a ch√≠nh li√™n quan (t·ª´ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t)
    vectorizer = CountVectorizer(stop_words='english', max_features=10)
    X = vectorizer.fit_transform(product_data['Noi_dung_binh_luan'])
    keywords = vectorizer.get_feature_names_out()

    print(f"\nC√°c t·ª´ kh√≥a ch√≠nh li√™n quan ƒë·∫øn s·∫£n ph·∫©m:")
    for keyword in keywords:
        print(f"- {keyword}")

    # Tr·ª±c quan h√≥a s·ªë sao
    plt.figure(figsize=(6, 4))
    product_data['So_sao'].value_counts().sort_index().plot(kind='bar', color='skyblue')
    plt.title(f"Ph√¢n ph·ªëi s·ªë sao cho s·∫£n ph·∫©m: {product_name}")
    plt.xlabel("S·ªë sao")
    plt.ylabel("S·ªë l∆∞·ª£ng nh·∫≠n x√©t")
    plt.xticks(rotation=0)
    plt.show()

# V√≠ d·ª• ph√¢n t√≠ch
product_id = '308500015'
product_name = 'M·∫∑t N·∫° Naruko B·∫°ch Ng·ªçc Lan D∆∞·ª°ng S√°ng, SƒÉn Ch·∫Øc Da 30ml'  # T√™n s·∫£n ph·∫©m

# G·ªçi h√†m ph√¢n t√≠ch s·∫£n ph·∫©m
analyze_product(product_id, product_name, final_data)

"""## 2. Ph√¢n t√≠ch c·∫£m x√∫c"""

def predict_sentiment(input_text, model, vectorizer, label_encoder):
    """
    D·ª± ƒëo√°n c·∫£m x√∫c t·ª´ vƒÉn b·∫£n ƒë·∫ßu v√†o.

    Args:
        input_text (str): VƒÉn b·∫£n ng∆∞·ªùi d√πng nh·∫≠p.
        model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán (Random Forest).
        vectorizer: B·ªô vectorizer ƒë√£ hu·∫•n luy·ªán.
        label_encoder: B·ªô m√£ h√≥a nh√£n (LabelEncoder).

    Returns:
        str: Nh√£n c·∫£m x√∫c d·ª± ƒëo√°n (Negative, Neutral, Positive).
    """
    # Bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√†nh vector
    input_vectorized = vectorizer.transform([input_text])

    # D·ª± ƒëo√°n nh√£n
    prediction = model.predict(input_vectorized)

    # Ki·ªÉm tra nh√£n ƒë·∫ßu ra
    if isinstance(prediction[0], str):  # N·∫øu nh√£n tr·∫£ v·ªÅ l√† chu·ªói
        prediction = label_encoder.transform(prediction)  # M√£ h√≥a l·∫°i sang nh√£n s·ªë

    # Gi·∫£i m√£ nh√£n t·ª´ s·ªë v·ªÅ chu·ªói
    sentiment = label_encoder.inverse_transform(prediction)[0]
    return sentiment

# Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_tfidf_resampled, y_train_resampled)

# D·ª± ƒëo√°n c·∫£m x√∫c t·ª´ input c·ªßa ng∆∞·ªùi d√πng
while True:
    user_input = input("Nh·∫≠p vƒÉn b·∫£n ƒë·ªÉ d·ª± ƒëo√°n c·∫£m x√∫c (ho·∫∑c g√µ 'exit' ƒë·ªÉ tho√°t): ")
    if user_input.lower() == 'exit':
        break
    try:
        sentiment = predict_sentiment(user_input, rf_model, tfidf_vectorizer, le)
        print(f"C·∫£m x√∫c d·ª± ƒëo√°n: {sentiment}")
    except ValueError as e:
        print(f"L·ªói d·ª± ƒëo√°n: {e}")





